{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlcodes (document):\n",
    "    \n",
    "    '''Removes HTML entity codes such as &amp from document and returns the clean document'''\n",
    "    \n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    \n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos (word):\n",
    "    \n",
    "    '''Returns the tag of usage of word depending on context'''\n",
    "    \n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop (str):\n",
    "    \n",
    "    '''Returns the lemmatized document after tokenization and stop word removal'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    \n",
    "    '''Returns true if text has no special characters, else returns false'''\n",
    "    \n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return TrueF\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words (text):\n",
    "    \n",
    "    '''Removes special characters from text and returns a clean string'''\n",
    "    \n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    \n",
    "    '''Cleans document_string by splitting very long strings and identifying garbage JSON and HTML and discarding'''\n",
    "    \n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "                    else:\n",
    "                        replace_with=' '.join(word for word in split)\n",
    "                        cleaned_doc = cleaned_doc.replace(word, replace_with)\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "count_dates = []\n",
    "\n",
    "def replace_dates(documentString, docID):\n",
    "    \n",
    "    '''Replaces dates of the format MM/DD and MM/DD/YYYY with DDmmmYYYY inside documentString'''\n",
    "    \n",
    "    regEx = '(([0-9]+(/)[0-9]+(/)[0-9]+)|([0-9]+(/)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    tmp = []\n",
    "    replace_with = []\n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp.append(date)\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            check_year = date[-3]\n",
    "            \n",
    "            if check_year == '/':\n",
    "                YY = date[-2:]\n",
    "                \n",
    "                if int(YY) <= 19:\n",
    "                    proper_date = date[:-2] + '20' + YY\n",
    "                    date = date.replace(date,proper_date)\n",
    "                else:\n",
    "                    proper_date = date[:-2] + '19' + YY\n",
    "                    date = date.replace(YY,('19'+YY))\n",
    "                    \n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        count_dates.append([docID, date])\n",
    "        newDate = newDate.replace(' ', '')\n",
    "        replace_with.append(newDate)\n",
    "        \n",
    "    for i in range(len(tmp)):\n",
    "        documentString = documentString.replace(tmp[i], replace_with[i])\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "# Reading persistent files\n",
    "\n",
    "import pickle\n",
    "import trie\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "data = np.load(\"datan.npy\", allow_pickle = True)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = data[i][0]\n",
    "    get_index[data[i][0]] = i\n",
    "collection = None\n",
    "documentRoot = {}\n",
    "max_tf = {}\n",
    "\n",
    "with open('collection.pickle', 'rb') as handle:\n",
    "    collection = pickle.load(handle)\n",
    "with open('documentRoot.pickle', 'rb') as handle:\n",
    "    documentRoot = pickle.load(handle)\n",
    "with open('max_tf.pickle', 'rb') as handle:\n",
    "    max_tf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laptop', 'india']\n"
     ]
    }
   ],
   "source": [
    "# Processing query\n",
    "\n",
    "import unidecode\n",
    "\n",
    "query = \"laptop india\"\n",
    "final_query = replace_dates(query, -1)\n",
    "final_query = lemma_stop(final_query)\n",
    "\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()\n",
    "print(final_query)\n",
    "\n",
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  laptop\n",
      "\n",
      "df =  3\n",
      "idf =  1.693140460675295\n",
      "-------------------------------------\n",
      "Term in query =  india\n",
      "\n",
      "df =  96\n",
      "idf =  0.18799048235538898\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443331-390\n",
      "Keywords:\n",
      "\n",
      "Lenovo Yoga Slim 7i Carbon With 11th-Gen Intel Core Processors, QHD Display Launched\n",
      "\n",
      "title score =  0\n",
      "laptop -1.5520454222856872 10\n",
      "india -0.10966111470731024 2\n",
      "\n",
      "\n",
      "Lenovo Yoga Slim 7i Carbon has been launched as the latest model in the company's Yoga series. The new laptop comes with a lightweight, 966 grams, chassis made of an aero-grade carbon fibre material that is touted to meet MIL-STD-810G standards and is designed to withstand knocks and bumps. The Yoga Slim 7i Carbon also comes Intel's Evo badge that is meant for premium ultraportable laptops to highlight their  ... \n",
      "\n",
      "tf-idf score= 3.1258875849738894\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443754-455\n",
      "Keywords:\n",
      "\n",
      "Huawei Said to Be in Talks to Sell Parts of Its Honor Smartphone Business\n",
      "\n",
      "title score =  0\n",
      "laptop -0.9070395325046222 1\n",
      "india -0.10742313277450799 2\n",
      "\n",
      "\n",
      "... smartphone business would be a win-win situation for the Honor brand, its suppliers and China's electronics industry.Honor Hunter V700 Gaming Laptop With 10th-Gen Intel Core CPUs Launched\"If Honor is independent from Huawei, its purchase of components will no longer be subject to the US ban on Huawei. This will help Honor's smartphone business and the suppliers,\" he wrote in a research note last week.The budget phone industry operates on thin  ... \n",
      "\n",
      "tf-idf score= 1.9083371102059512\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443357-396\n",
      "Keywords:\n",
      "\n",
      "Big Tech Firms on EU 'Hit List' Could Face Tougher Regulations: Report\n",
      "\n",
      "title score =  0\n",
      "laptop -0.967508834671597 1\n",
      "\n",
      "\n",
      "... The report had said that companies should not be allowed to pre-install their own applications on hardware devices, such as laptops or phones, or force other companies to exclusively pre-install their software.When reached out for comments by Reuters, Goole had pointed to a blog post it had published in late September, which details its response to the draft regulations.\"The question is not whether data mobility or data access should be  ... \n",
      "\n",
      "tf-idf score= 1.6381283540432854\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443383-401\n",
      "Keywords:\n",
      "\n",
      "Apple Diwali Offer: Free AirPods With iPhone 11 Purchases via Apple Store Online Starting October 17 in India\n",
      "\n",
      "title score =  0\n",
      "india -0.1488257985313496 7\n",
      "\n",
      "\n",
      "Apple has announced an offer for its customers in India, as a part of which, they can get free AirPods on the purchase of iPhone 11 from its online store. This Diwali offer is set to go live October 17, and is only applicable on purchases via Apple's recently launched online store in India. The AirPods that will be  ... \n",
      "\n",
      "tf-idf score= 0.027977833652834357\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443878-489\n",
      "Keywords:\n",
      "\n",
      "Tata Group Looking for Stake in IPO-Bound Online Grocer BigBasket: Report\n",
      "\n",
      "title score =  0\n",
      "india -0.1315933376487723 2\n",
      "\n",
      "\n",
      "... consumer businesses, several media reports have said, as it competes against Amazon and Reliance, who have made big bets on India's booming e-commerce market.Bengaluru-based BigBasket is looking to raise $200 million (roughly Rs. 1,465 crores) for a fresh funding round, which could potentially value the startup at nearly $2 billion (roughly Rs. 14,652 crores), according to the report.Amazon, BigBasket Get Nod to Begin Home Delivery of Alcohol in West BengalIt  ... \n",
      "\n",
      "tf-idf score= 0.024738295019348273\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443916-501\n",
      "Keywords:\n",
      "\n",
      "Amazfit Bip U Smartwatch With Over 50 Watch Faces, 60 Sports Modes Launched in India\n",
      "\n",
      "title score =  0\n",
      "india -0.1301472570152693 5\n",
      "\n",
      "\n",
      "Amazfit Bip U smartwatch has launched in India as the latest offering from the company. The wearable is touted by the company to offer up to nine days of battery life and come with over 60 sports modes. The Amazfit Bip U comes with more than 50 watch faces for display customisation and offers tracking of stress,  ... \n",
      "\n",
      "tf-idf score= 0.024466445623531258\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443922-503\n",
      "Keywords:\n",
      "\n",
      "Mi True Wireless Earphones 2C With 14.2mm Drivers, Environmental Noise Cancellation Launched in India\n",
      "\n",
      "title score =  0\n",
      "india -0.12532698823692598 4\n",
      "\n",
      "\n",
      "Mi True Wireless Earphones 2C have been launched in India as the latest offering from the company. Xiaomi has introduced the TWS earbuds just ahead of the Flipkart Big Billion Days sale in the country. The earbuds were launched alongside the Mi 10T and Mi 10T Pro phones. The Mi True Wireless Earphones 2C come with up to 20  ... \n",
      "\n",
      "tf-idf score= 0.023560280970807874\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443367-398\n",
      "Keywords:\n",
      "\n",
      "Vivo V20 Price in India Tipped Ahead of Tuesday Launch\n",
      "\n",
      "title score =  0\n",
      "india -0.12532698823692598 5\n",
      "\n",
      "\n",
      "Vivo V20 price in India has been leaked just ahead of its official launch that is set for Tuesday, October 13. The Vivo phone was unveiled globally at an event in Thailand last month â€” alongside the Vivo V20 Pro. However, its specifications were detailed just a couple of weeks back. The Vivo V20  ... \n",
      "\n",
      "tf-idf score= 0.023560280970807874\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443902-497\n",
      "Keywords:\n",
      "\n",
      "Oppo A15 With Triple Rear Cameras, MediaTek Helio P35 SoC Launched in India: Price, Specifications\n",
      "\n",
      "title score =  0\n",
      "india -0.12532698823692598 3\n",
      "\n",
      "\n",
      "Oppo A15 has been launched in India as the company's latest camera-focussed budget smartphone. The new Oppo phone comes with triple rear cameras that include a 13-megapixel primary sensor and is backed by artificial intelligence (AI) powered features. The smartphone also features a waterdrop-style display notch. Oppo has claimed to offer proprietary eye protection on the  ... \n",
      "\n",
      "tf-idf score= 0.023560280970807874\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443312-384\n",
      "Keywords:\n",
      "\n",
      "Oppo A15 Teased to Sport 6.52-Inch Display, Key Specifications Leaked\n",
      "\n",
      "title score =  0\n",
      "india -0.12532698823692598 3\n",
      "\n",
      "\n",
      "Oppo A15 is all set to launch in India soon. The phone is being teased on Amazon India and the latest teaser hints at the display credentials of the phone. The Oppo A15 is teased to sport a 6.52-inch display with a waterdrop-style notch. The phone is also teased to have eye protection and AI brightness that learns  ... \n",
      "\n",
      "tf-idf score= 0.023560280970807874\n",
      "\n",
      "\n",
      "============================================\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7b1143daf0be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0msplit_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",|\\s\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msorted_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_l\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0msplit_l\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msplit_l\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;31m#print(s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    \n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        \n",
    "        tf_doc = documentRoot[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        tfidf_doc = (tf_doc)\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "            \n",
    "# print(title_score)\n",
    "\n",
    "for docID in scores:\n",
    "    \n",
    "    #if documentLength[docID] != 0:\n",
    "    scores[docID] *= factor[docID]\n",
    "    if docID in title_score:\n",
    "        scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    \n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(data[get_index[sorted_scores[i][0]]][2])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(documentRoot[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    \n",
    "    for word in data[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word, -1)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('============================================')\n",
    "#print(sorted_scores)\n",
    "dates = []\n",
    "from collections import Counter\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    split_l = re.split(\",|\\s\",data[get_index[sorted_scores[i][0]]][5])\n",
    "    s = split_l[0] +\" \"+ split_l[1] + \" \" + split_l[3]\n",
    "    #print(s)\n",
    "    dates.append(s)\n",
    "    #print(dates)\n",
    "    \n",
    "#dates.sort(key = lambda date: datetime.strptime(date, '%B %d %Y')) \n",
    "#print(dates)\n",
    "count1 = 0\n",
    "for keys in Counter(dates).most_common():\n",
    "    print(keys[0],keys[1])\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
