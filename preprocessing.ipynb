{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhRPXd06Hkpo",
    "outputId": "57b3984e-bc49-4f3b-a3d3-d29ca48caa29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordninja in c:\\users\\mypc\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qZ-TMc9SVHl",
    "outputId": "00fe7e36-6884-4f96-cd08-9a31c65e3a21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MYPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MYPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MYPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja\n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "###pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn-b0w8-hmKn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3H4pv21qhRaW"
   },
   "outputs": [],
   "source": [
    "def remove_htmlcodes(document):\n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sports.csv\")\n",
    "data = np.array(data)\n",
    "np.save(\"dataset\",data)\n",
    "data = np.load(\"dataset.npy\",allow_pickle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6aQ7ZcPCH4d",
    "outputId": "6d821e4d-31d2-4b2f-f556-ea116a8ec159",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "\n",
    "NN = len(data)\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = data[i][0]\n",
    "    get_index[data[i][0]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "790C4ZoWhRaX"
   },
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "gZeBOk8PhRaX"
   },
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "4wnQZK3EhRaX"
   },
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "H1_nZ_FvhRaX"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp = date\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        documentString = documentString.replace(tmp, newDate)\n",
    "        # print(newDate)\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbw8tNbrJfL"
   },
   "source": [
    "*Run the following cell once after all pre-processing (removing JSON etc), and store final lemmatized contents of all docs:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "JokFmLV8hRaX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59575\n"
     ]
    }
   ],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == NN:\n",
    "        break\n",
    "print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRqA8lwehRaX",
    "outputId": "11ac5ec9-384f-49ba-db1e-bd1eb4e9ec2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59575/59575 [50:27<00:00, 19.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3028.0473091602325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    # actually modifying the document\n",
    "    document[4] = remove_htmlcodes(document[4])\n",
    "    \n",
    "    # not actually modifying the document\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(clean_document(modifiedContent))\n",
    "    modifiedTitle = lemma_stop(clean_document(document[2]))\n",
    "    \n",
    "    # case-folding\n",
    "    for i in range(len(modifiedContent)):\n",
    "        modifiedContent[i] = modifiedContent[i].lower()\n",
    "    \n",
    "    # modifiedTitle = lemma_stop((document[1]))\n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSWPALO7jzOb",
    "outputId": "225faea6-b365-4a25-a5ee-07122fd5b2a3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "5l8HSfuohRaX"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "\n",
    "titles_temp = titles\n",
    "\n",
    "for i in range(NN):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "import pickle \n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "9M63H5wohRaX"
   },
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(NN):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9cGiLfuhRaY",
    "outputId": "639395aa-9502-491e-85dd-e6e6c60dbc7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59575/59575 [01:22<00:00, 722.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.5031087398529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "max_tf = {}\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(NN)):\n",
    "    for w in contents_temp[i]:\n",
    "        collection.add_document(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "        if get_docID[i] in max_tf:\n",
    "            max_tf[get_docID[i]] = max(documentRoot[i].count_words(w, 0), max_tf[get_docID[i]])\n",
    "        else:\n",
    "            max_tf[get_docID[i]] = documentRoot[i].count_words(w, 0)\n",
    "    for w in titles_temp[i]:\n",
    "        collection.add_title(w, 0, get_docID[i])\n",
    "        \n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eB9AxokfhRaY",
    "outputId": "de59d2a7-8c74-449b-cbbb-8cd144b28630"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/59575 [00:00<13:18, 74.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "59575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59575/59575 [12:03<00:00, 82.40it/s] \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import queue\n",
    "\n",
    "documentLength = {}\n",
    "N = len(documentRoot)\n",
    "print(\"hello\")\n",
    "print(N)\n",
    "\n",
    "for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "    docID = get_docID[i]\n",
    "    length = 0\n",
    "    document = documentRoot[i]\n",
    "    q = queue.Queue()\n",
    "    q.put([document, ''])\n",
    "    \n",
    "    while q.qsize() > 0:\n",
    "\n",
    "        current = q.get()\n",
    "        reference = current[0]\n",
    "        word = current[1]\n",
    "\n",
    "        if reference.words > 0:\n",
    "            df = len(collection.get_doc_list(word, 0))\n",
    "            idf = math.log10(N/df)\n",
    "            # print(word, reference.words, df)\n",
    "            length += (reference.words * idf) ** 2\n",
    "\n",
    "        for i in range(256):\n",
    "            if reference.children.get(i) is not None:\n",
    "                new_word = word + chr(i)\n",
    "                q.put([reference.children[i], new_word])\n",
    "\n",
    "    # print(length**0.5)\n",
    "    documentLength[docID] = length**0.5\n",
    "    \n",
    "    \n",
    "    \n",
    "pickle_out = open(\"collection.pickle\",\"wb\")\n",
    "pickle.dump(collection,pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out1 = open(\"documentRoot.pickle\",\"wb\")\n",
    "pickle.dump(getReference,pickle_out1)\n",
    "pickle_out1.close()\n",
    "pickle_out2 = open(\"max_tf.pickle\",\"wb\")\n",
    "pickle.dump(max_tf,pickle_out2)\n",
    "pickle_out2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auUhJ1AyhRaY",
    "outputId": "e8bb2c75-3e7d-4ded-9055-df3b9f484c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514560\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(documentRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go9w6bB9rJfg",
    "outputId": "ce75612b-2151-4719-c11f-709572092d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tennis\n",
      "['tennis']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    \n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()\n",
    "print(final_query)\n",
    "print(len(final_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "j4Kqgk9hrJfj"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "JVFuxoqchRaY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfT-kdUqrJfp",
    "outputId": "956f82e5-e660-4a44-8820-162fe24d5254",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  tennis\n",
      "\n",
      "{'1607985258-59209', '1607985243-59133', '1607985163-58710', '1607985131-58535', '1607984673-56153', '1607984082-53249', '1607984294-54221', '1607985276-59317', '1607984930-57502', '1607984440-54992', '1607983918-52480', '1607984294-54219', '1607984731-56453', '1607985072-58213', '1607984510-55353', '1607985272-59276', '1607985122-58484', '1607984230-53876', '1607985016-57940', '1607985021-57974', '1607984247-53958', '1607985258-59210', '1607985317-59514', '1607985182-58813', '1607983853-52163', '1607985285-59365', '1607985268-59251', '1607984996-57843', '1607985211-58969', '1607985040-58052', '1607985086-58283', '1607985272-59300', '1607984266-54064', '1607984920-57460', '1607962354-10075', '1607985312-59478', '1607984179-53668', '1607984950-57607', '1607985272-59277', '1607984477-55200', '1607984066-53158', '1607985268-59261', '1607984632-55965', '1607984977-57734', '1607984033-53038', '1607984321-54359', '1607985224-59030', '1607984166-53621', '1607985141-58600', '1607985258-59206', '1607984190-53720', '1607983867-52235', '1607964898-15360', '1607984416-54862', '1607984271-54084', '1607984504-55348', '1607984006-52910', '1607984936-57526', '1607984688-56245', '1607985187-58843', '1607960329-9064', '1607984805-56866', '1607984128-53448', '1607984303-54262', '1607985000-57874', '1607984294-54215', '1607985127-58518', '1607985239-59106', '1607984510-55357', '1607970382-34146', '1607984230-53898', '1607984477-55182', '1607985082-58261', '1607984058-53145', '1607971513-38020', '1607984608-55841', '1607984308-54289', '1607985268-59268', '1607984664-56118', '1607984040-53056', '1607973988-43098', '1607984805-56871', '1607985122-58494', '1607984271-54096', '1607984308-54292', '1607985320-59529', '1607983992-52842', '1607985141-58583', '1607985114-58430', '1607985046-58088', '1607984128-53450', '1607964242-13617', '1607984058-53138', '1607985155-58667', '1607985258-59203', '1607984377-54653', '1607984040-53053', '1607984653-56057', '1607985254-59186', '1607984701-56316', '1607984740-56510', '1607984046-53079', '1607984271-54081', '1607985248-59157', '1607984930-57513', '1607985258-59215', '1607984154-53561', '1607985146-58622', '1607984530-55451', '1607984707-56346', '1607984602-55814', '1607983894-52362', '1607984391-54747', '1607985196-58895', '1607984712-56351', '1607983913-52474', '1607985262-59239', '1607969327-30499', '1607985211-58973', '1607985248-59156', '1607985324-59557', '1607984247-53975', '1607984420-54897', '1607984308-54283', '1607984308-54285', '1607984664-56107', '1607984312-54317', '1607984664-56125', '1607985281-59329', '1607984659-56097', '1607984983-57775', '1607984495-55297', '1607985285-59359', '1607984303-54258', '1607984950-57608', '1607983867-52249', '1607984597-55798', '1607985146-58603', '1607985258-59222', '1607983903-52408', '1607985067-58199', '1607984790-56777', '1607984046-53084', '1607984015-52953', '1607984809-56890', '1607984303-54274', '1607984367-54612', '1607985290-59392', '1607984107-53330', '1607984782-56738', '1607985320-59544', '1607984357-54551', '1607985243-59140', '1607983853-52170', '1607984867-57198', '1607959082-6777', '1607984926-57476', '1607984349-54509', '1607985248-59175', '1607984608-55833', '1607985136-58566', '1607983885-52307', '1607983982-52778', '1607985016-57935', '1607984673-56163', '1607985086-58286', '1607985040-58074', '1607985168-58750', '1607984983-57770', '1607984458-55089', '1607984967-57693', '1607984707-56332', '1607984472-55160', '1607959539-8380', '1607971821-39143', '1607984735-56493', '1607984308-54288', '1607985072-58220', '1607984400-54794', '1607984991-57803', '1607984632-55970', '1607984077-53213', '1607984033-53030', '1607985258-59201', '1607984303-54257', '1607985272-59288', '1607984647-56029', '1607984330-54403', '1607984566-55648', '1607985077-58248', '1607983873-52263', '1607985010-57918', '1607984795-56801', '1607984582-55720', '1607984977-57728', '1607985308-59468', '1607985234-59081', '1607985272-59283', '1607984006-52901', '1607968726-28704', '1607985141-58598', '1607984735-56494', '1607984602-55803', '1607984561-55616', '1607984510-55370', '1607983918-52495', '1607984072-53193', '1607985308-59463', '1607984901-57359', '1607964369-13876', '1607983873-52256', '1607985239-59123', '1607974726-47267', '1607985254-59193', '1607984266-54060', '1607984814-56925', '1607985187-58850', '1607985192-58872', '1607985268-59270', '1607984033-53034', '1607985303-59426', '1607984426-54901', '1607985000-57856', '1607985010-57908', '1607984173-53629', '1607984237-53918', '1607985248-59151', '1607984829-56994', '1607984330-54424'}\n",
      "{'1607985258-59209': 1.9621506943756382, '1607985243-59133': 1.9621506943756382, '1607985163-58710': 1.9621506943756382, '1607985131-58535': 1.9621506943756382, '1607984673-56153': 1.9621506943756382, '1607984082-53249': 1.9621506943756382, '1607984294-54221': 1.9621506943756382, '1607985276-59317': 1.9621506943756382, '1607984930-57502': 1.9621506943756382, '1607984440-54992': 1.9621506943756382, '1607983918-52480': 1.9621506943756382, '1607984294-54219': 1.9621506943756382, '1607984731-56453': 1.9621506943756382, '1607985072-58213': 1.9621506943756382, '1607984510-55353': 1.9621506943756382, '1607985272-59276': 1.9621506943756382, '1607985122-58484': 1.9621506943756382, '1607984230-53876': 1.9621506943756382, '1607985016-57940': 1.9621506943756382, '1607985021-57974': 1.9621506943756382, '1607984247-53958': 1.9621506943756382, '1607985258-59210': 1.9621506943756382, '1607985317-59514': 1.9621506943756382, '1607985182-58813': 1.9621506943756382, '1607983853-52163': 1.9621506943756382, '1607985285-59365': 1.9621506943756382, '1607985268-59251': 1.9621506943756382, '1607984996-57843': 1.9621506943756382, '1607985211-58969': 1.9621506943756382, '1607985040-58052': 1.9621506943756382, '1607985086-58283': 1.9621506943756382, '1607985272-59300': 1.9621506943756382, '1607984266-54064': 1.9621506943756382, '1607984920-57460': 1.9621506943756382, '1607962354-10075': 1.9621506943756382, '1607985312-59478': 1.9621506943756382, '1607984179-53668': 1.9621506943756382, '1607984950-57607': 1.9621506943756382, '1607985272-59277': 1.9621506943756382, '1607984477-55200': 1.9621506943756382, '1607984066-53158': 1.9621506943756382, '1607985268-59261': 1.9621506943756382, '1607984632-55965': 1.9621506943756382, '1607984977-57734': 1.9621506943756382, '1607984033-53038': 1.9621506943756382, '1607984321-54359': 1.9621506943756382, '1607985224-59030': 1.9621506943756382, '1607984166-53621': 1.9621506943756382, '1607985141-58600': 1.9621506943756382, '1607985258-59206': 1.9621506943756382, '1607984190-53720': 1.9621506943756382, '1607983867-52235': 1.9621506943756382, '1607964898-15360': 1.9621506943756382, '1607984416-54862': 1.9621506943756382, '1607984271-54084': 1.9621506943756382, '1607984504-55348': 1.9621506943756382, '1607984006-52910': 1.9621506943756382, '1607984936-57526': 1.9621506943756382, '1607984688-56245': 1.9621506943756382, '1607985187-58843': 1.9621506943756382, '1607960329-9064': 1.9621506943756382, '1607984805-56866': 1.9621506943756382, '1607984128-53448': 1.9621506943756382, '1607984303-54262': 1.9621506943756382, '1607985000-57874': 1.9621506943756382, '1607984294-54215': 1.9621506943756382, '1607985127-58518': 1.9621506943756382, '1607985239-59106': 1.9621506943756382, '1607984510-55357': 1.9621506943756382, '1607970382-34146': 1.9621506943756382, '1607984230-53898': 1.9621506943756382, '1607984477-55182': 1.9621506943756382, '1607985082-58261': 1.9621506943756382, '1607984058-53145': 1.9621506943756382, '1607971513-38020': 1.9621506943756382, '1607984608-55841': 1.9621506943756382, '1607984308-54289': 1.9621506943756382, '1607985268-59268': 1.9621506943756382, '1607984664-56118': 1.9621506943756382, '1607984040-53056': 1.9621506943756382, '1607973988-43098': 1.9621506943756382, '1607984805-56871': 1.9621506943756382, '1607985122-58494': 1.9621506943756382, '1607984271-54096': 1.9621506943756382, '1607984308-54292': 1.9621506943756382, '1607985320-59529': 1.9621506943756382, '1607983992-52842': 1.9621506943756382, '1607985141-58583': 1.9621506943756382, '1607985114-58430': 1.9621506943756382, '1607985046-58088': 1.9621506943756382, '1607984128-53450': 1.9621506943756382, '1607964242-13617': 1.9621506943756382, '1607984058-53138': 1.9621506943756382, '1607985155-58667': 1.9621506943756382, '1607985258-59203': 1.9621506943756382, '1607984377-54653': 1.9621506943756382, '1607984040-53053': 1.9621506943756382, '1607984653-56057': 1.9621506943756382, '1607985254-59186': 1.9621506943756382, '1607984701-56316': 1.9621506943756382, '1607984740-56510': 1.9621506943756382, '1607984046-53079': 1.9621506943756382, '1607984271-54081': 1.9621506943756382, '1607985248-59157': 1.9621506943756382, '1607984930-57513': 1.9621506943756382, '1607985258-59215': 1.9621506943756382, '1607984154-53561': 1.9621506943756382, '1607985146-58622': 1.9621506943756382, '1607984530-55451': 1.9621506943756382, '1607984707-56346': 1.9621506943756382, '1607984602-55814': 1.9621506943756382, '1607983894-52362': 1.9621506943756382, '1607984391-54747': 1.9621506943756382, '1607985196-58895': 1.9621506943756382, '1607984712-56351': 1.9621506943756382, '1607983913-52474': 1.9621506943756382, '1607985262-59239': 1.9621506943756382, '1607969327-30499': 1.9621506943756382, '1607985211-58973': 1.9621506943756382, '1607985248-59156': 1.9621506943756382, '1607985324-59557': 1.9621506943756382, '1607984247-53975': 1.9621506943756382, '1607984420-54897': 1.9621506943756382, '1607984308-54283': 1.9621506943756382, '1607984308-54285': 1.9621506943756382, '1607984664-56107': 1.9621506943756382, '1607984312-54317': 1.9621506943756382, '1607984664-56125': 1.9621506943756382, '1607985281-59329': 1.9621506943756382, '1607984659-56097': 1.9621506943756382, '1607984983-57775': 1.9621506943756382, '1607984495-55297': 1.9621506943756382, '1607985285-59359': 1.9621506943756382, '1607984303-54258': 1.9621506943756382, '1607984950-57608': 1.9621506943756382, '1607983867-52249': 1.9621506943756382, '1607984597-55798': 1.9621506943756382, '1607985146-58603': 1.9621506943756382, '1607985258-59222': 1.9621506943756382, '1607983903-52408': 1.9621506943756382, '1607985067-58199': 1.9621506943756382, '1607984790-56777': 1.9621506943756382, '1607984046-53084': 1.9621506943756382, '1607984015-52953': 1.9621506943756382, '1607984809-56890': 1.9621506943756382, '1607984303-54274': 1.9621506943756382, '1607984367-54612': 1.9621506943756382, '1607985290-59392': 1.9621506943756382, '1607984107-53330': 1.9621506943756382, '1607984782-56738': 1.9621506943756382, '1607985320-59544': 1.9621506943756382, '1607984357-54551': 1.9621506943756382, '1607985243-59140': 1.9621506943756382, '1607983853-52170': 1.9621506943756382, '1607984867-57198': 1.9621506943756382, '1607959082-6777': 1.9621506943756382, '1607984926-57476': 1.9621506943756382, '1607984349-54509': 1.9621506943756382, '1607985248-59175': 1.9621506943756382, '1607984608-55833': 1.9621506943756382, '1607985136-58566': 1.9621506943756382, '1607983885-52307': 1.9621506943756382, '1607983982-52778': 1.9621506943756382, '1607985016-57935': 1.9621506943756382, '1607984673-56163': 1.9621506943756382, '1607985086-58286': 1.9621506943756382, '1607985040-58074': 1.9621506943756382, '1607985168-58750': 1.9621506943756382, '1607984983-57770': 1.9621506943756382, '1607984458-55089': 1.9621506943756382, '1607984967-57693': 1.9621506943756382, '1607984707-56332': 1.9621506943756382, '1607984472-55160': 1.9621506943756382, '1607959539-8380': 1.9621506943756382, '1607971821-39143': 1.9621506943756382, '1607984735-56493': 1.9621506943756382, '1607984308-54288': 1.9621506943756382, '1607985072-58220': 1.9621506943756382, '1607984400-54794': 1.9621506943756382, '1607984991-57803': 1.9621506943756382, '1607984632-55970': 1.9621506943756382, '1607984077-53213': 1.9621506943756382, '1607984033-53030': 1.9621506943756382, '1607985258-59201': 1.9621506943756382, '1607984303-54257': 1.9621506943756382, '1607985272-59288': 1.9621506943756382, '1607984647-56029': 1.9621506943756382, '1607984330-54403': 1.9621506943756382, '1607984566-55648': 1.9621506943756382, '1607985077-58248': 1.9621506943756382, '1607983873-52263': 1.9621506943756382, '1607985010-57918': 1.9621506943756382, '1607984795-56801': 1.9621506943756382, '1607984582-55720': 1.9621506943756382, '1607984977-57728': 1.9621506943756382, '1607985308-59468': 1.9621506943756382, '1607985234-59081': 1.9621506943756382, '1607985272-59283': 1.9621506943756382, '1607984006-52901': 1.9621506943756382, '1607968726-28704': 1.9621506943756382, '1607985141-58598': 1.9621506943756382, '1607984735-56494': 1.9621506943756382, '1607984602-55803': 1.9621506943756382, '1607984561-55616': 1.9621506943756382, '1607984510-55370': 1.9621506943756382, '1607983918-52495': 1.9621506943756382, '1607984072-53193': 1.9621506943756382, '1607985308-59463': 1.9621506943756382, '1607984901-57359': 1.9621506943756382, '1607964369-13876': 1.9621506943756382, '1607983873-52256': 1.9621506943756382, '1607985239-59123': 1.9621506943756382, '1607974726-47267': 1.9621506943756382, '1607985254-59193': 1.9621506943756382, '1607984266-54060': 1.9621506943756382, '1607984814-56925': 1.9621506943756382, '1607985187-58850': 1.9621506943756382, '1607985192-58872': 1.9621506943756382, '1607985268-59270': 1.9621506943756382, '1607984033-53034': 1.9621506943756382, '1607985303-59426': 1.9621506943756382, '1607984426-54901': 1.9621506943756382, '1607985000-57856': 1.9621506943756382, '1607985010-57908': 1.9621506943756382, '1607984173-53629': 1.9621506943756382, '1607984237-53918': 1.9621506943756382, '1607985248-59151': 1.9621506943756382, '1607984829-56994': 1.9621506943756382, '1607984330-54424': 1.9621506943756382}\n",
      "\n",
      "\n",
      "\n",
      "===========================================================================================================\n",
      "\n",
      "doc ID =  1607985243-59133\n",
      "Keywords:\n",
      "\n",
      "Putting together the perfect tennis player with Mary Pierce\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "From fitness of Martina Navratilova to the heart of Monica Seles, here's what Mary Pierce's perfect tennis player looks like.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607984510-55353\n",
      "Keywords:\n",
      "\n",
      "Five-set future in tennis up for debate in hashtag age\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board of governing body the International Tennis Federation wants to cut men's singles matches in the Davis Cup team competition to best-of-three sets, and members will be asked to approve the change in August.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607984308-54292\n",
      "Keywords:\n",
      "\n",
      "India’s Somdev Devvarman announces retirement from pro tennis\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "Somdev Devvarman announces retirement from pro tennis after the 31-year old decided to give Chennai Open a miss.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607985258-59203\n",
      "Keywords:\n",
      "\n",
      "Small exhibition events testing the waters as tennis plots way back onto court\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 2\n",
      "\n",
      "\n",
      "The BASE Tennis Academy in the town of Höhr-Grenzhausen hosted a four-day Tennis Point Exhibition Series – an eight-man event.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607984046-53079\n",
      "Keywords:\n",
      "\n",
      "Chris Evert stunned by Maria Sharapova news; not by doping in tennis\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "Maria Sharapova faces up to a four-year ban by the International Tennis Federation for testing positive for meldonium.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607985248-59157\n",
      "Keywords:\n",
      "\n",
      "With COVID-19 at large, professional tennis events face an existential threat\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "With professional tennis on hold until at least June, the sport’s administrators and players are scrambling to cut their losses as tournaments are postponed or cancelled en masse.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607985258-59215\n",
      "Keywords:\n",
      "\n",
      "Rafael Nadal pessimistic over chances of return to normal for tennis\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "Professional tennis has been suspended until the end of July, at the earliest, and the Spaniard said on Sunday that serious problems stand in the way of a resumption.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607985146-58622\n",
      "Keywords:\n",
      "\n",
      "WATCH: American tennis player Alison Riske dedicates Bollywood dance number to Indian followers\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "The wedding was held in Pittsburgh but the highlight of the event was the tennis star and her sister Sarah dancing on a Bollywood number.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607959539-8380\n",
      "Keywords:\n",
      "\n",
      "Use cricket stadiums for other games like hockey, tennis: Lodha panel to BCCI\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "BCCI functionaries have questioned the logic behind laying astro-turf or a synthetic tennis court in a cricket stadium.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n",
      "\n",
      "doc ID =  1607985272-59288\n",
      "Keywords:\n",
      "\n",
      "When tennis returns: No fans, less prize money, rustiness\n",
      "\n",
      "title score =  1.9621506943756382\n",
      "tennis -1.9621506943756382 1\n",
      "\n",
      "\n",
      "As tennis makes a comeback after the Covid-induced hiatus, there won’t be fans at the U.S. Open or likely anywhere else at the outset, and the players will be vying for decreased prize money.  ... \n",
      "\n",
      "tf-idf score= 11.40438487778659\n",
      "\n",
      "\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    \n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    # print('List of documents with this term=', docs_having_query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    print(docs_having_query_term_in_title)\n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "#     print('df = ',df)\n",
    "#     print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        # print(docID)\n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        # print('tf for doc',docID,'is',tf_doc)\n",
    "        # tfidf_doc_query = tf_doc * idf\n",
    "        # tfidf_doc = 1 + math.log10(tf_doc)\n",
    "        tfidf_doc = (tf_doc)\n",
    "        # tfidf_doc_query = (tf_doc)\n",
    "        \n",
    "        # print('tfidf for doc',doc,'is',tfidf_doc)\n",
    "        # print()\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "print(title_score)\n",
    "for docID in scores:\n",
    "    if documentLength[docID] != 0:\n",
    "#         scores[docID] = scores[docID]/ math.sqrt(documentLength[docID])\n",
    "        scores[docID] *= factor[docID]\n",
    "        if docID in title_score:\n",
    "            scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('===========================================================================================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    # print(i)\n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(subset[get_index[sorted_scores[i][0]]][2])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(getReference[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    for word in subset[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('=======================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
