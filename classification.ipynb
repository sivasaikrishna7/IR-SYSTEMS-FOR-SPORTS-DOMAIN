{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELTn8hMgzqgM"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6zuTeel0mzp"
      },
      "source": [
        "football= pd.read_csv(\"/content/drive/MyDrive/footballsowmya.csv\")\r\n",
        "cricket = pd.read_csv(\"/content/drive/MyDrive/cricketsowmya.csv\")\r\n",
        "tennis = pd.read_csv(\"/content/drive/MyDrive/tennisssowmya.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibDqLsZBCW0O"
      },
      "source": [
        "football = football['title']\r\n",
        "cricket = cricket['title']\r\n",
        "tennis = tennis['title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYhcaUIICbeA"
      },
      "source": [
        "football = football.to_frame()\r\n",
        "football['domain'] = 'football'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoGoICy98SST"
      },
      "source": [
        "tennis = tennis.to_frame()\r\n",
        "tennis['domain'] = 'tennis'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62YmCdcTCdpf"
      },
      "source": [
        "cricket = cricket.to_frame()\r\n",
        "cricket['domain'] = 'cricket'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_MchLl1Cfba"
      },
      "source": [
        "concatenated = pd.concat([football, cricket])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGCBWHkWCiMW"
      },
      "source": [
        "final = pd.concat([concatenated, tennis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrrh4g_sCkR6"
      },
      "source": [
        "final.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCDrAVu1Cl0C"
      },
      "source": [
        "final.to_csv('/content/drive/MyDrive/IR_dataset_for_preprossing.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQiaYSZpC5tA"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/IR_dataset_for_preprossing.csv\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNIWhmb1INUc"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Ci9mfQ8fhs"
      },
      "source": [
        "pip install emot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WQs7wjXDQaF"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk import sent_tokenize, word_tokenize\r\n",
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re     \r\n",
        "import re\r\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rx6_GoCJV-J"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6blfMuyJWBA"
      },
      "source": [
        "# nlp = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WabSpjUAJr7T"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O82QgcDd9Zu9"
      },
      "source": [
        "pip install word2number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CfNb8PiJiun"
      },
      "source": [
        "pip install unidecode\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwcCTapxJaiw"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import spacy\r\n",
        "import unidecode\r\n",
        "from word2number import w2n\r\n",
        "import gensim.downloader as api\r\n",
        "import en_core_web_md\r\n",
        "import contractions\r\n",
        "import pandas as pd\r\n",
        "from itertools import count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWX2yxN-9tOr"
      },
      "source": [
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5GAFvu0EQRE"
      },
      "source": [
        "def remove_whitespace(text):\r\n",
        "    \"\"\"remove extra whitespaces from text\"\"\"\r\n",
        "    text = text.strip()\r\n",
        "    return \" \".join(text.split())\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def expand_contractions(text):\r\n",
        "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\r\n",
        "    text = contractions.fix(text)\r\n",
        "#     text = list(cont.expand_texts([text], precise=True))[0]\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "def text_preprocessing(text, contractions=False, \r\n",
        "                       convert_num=False, extra_whitespace=False, \r\n",
        "                       lemmatization=False, lowercase=False, punctuations=False,\r\n",
        "                       remove_num=False, special_chars=True, \r\n",
        "                       stop_words=False):\r\n",
        "    \"\"\"preprocess text with default option set to true for all steps\"\"\"\r\n",
        "    \r\n",
        "    text = str(text)\r\n",
        "    \r\n",
        "    if extra_whitespace == True: #remove extra whitespaces\r\n",
        "        text = remove_whitespace(text)\r\n",
        "    if contractions == True: #expand contractions\r\n",
        "        text = expand_contractions(text)\r\n",
        "    if lowercase == True: #convert all characters to lowercase\r\n",
        "        text = text.lower()\r\n",
        "    \r\n",
        "\r\n",
        "    doc = nlp(text) #tokenise text\r\n",
        "\r\n",
        "    clean_text = []\r\n",
        "    ct = ''\r\n",
        "    \r\n",
        "    for token in doc:\r\n",
        "        flag = True\r\n",
        "        edit = token.text\r\n",
        "        # remove stop words\r\n",
        "        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \r\n",
        "            flag = False\r\n",
        "        # remove punctuations\r\n",
        "        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \r\n",
        "            flag = False\r\n",
        "        # remove special characters\r\n",
        "        if special_chars == True and token.pos_ == 'SYM' and flag == True: \r\n",
        "            flag = False\r\n",
        "        # remove numbers\r\n",
        "        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\r\n",
        "        and flag == True:\r\n",
        "            flag = False\r\n",
        "\r\n",
        "        # convert number words to numeric numbers\r\n",
        "        if convert_num == True and token.pos_ == 'NUM' and flag == True:\r\n",
        "            edit = w2n.word_to_num(token.text)\r\n",
        "        # convert tokens to base form\r\n",
        "        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\r\n",
        "            edit = token.lemma_\r\n",
        "        # append tokens edited and not removed to list \r\n",
        "        if edit != \"\" and flag == True:\r\n",
        "            clean_text.append(edit)    \r\n",
        "     \r\n",
        "    for i in clean_text:\r\n",
        "      ct += i+' '\r\n",
        "    ct = remove_whitespace(ct)\r\n",
        "    return ct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "468T1d48HdNK"
      },
      "source": [
        "def preprocess_and_save(ds):\r\n",
        "  #ds = ds.astype(str)\r\n",
        "  ds['title'] = ds['title'].map(text_preprocessing)\r\n",
        "  \r\n",
        "  ds.to_csv(\"/content/drive/MyDrive/IR_domain_custom_dataset_processed.csv\")\r\n",
        "  print(\"save successful\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctwk6hMcIAk4"
      },
      "source": [
        "preprocess_and_save(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsFyDmnFIGvG"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNGM7_qkNpeJ"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/IR_domain_custom_dataset_processed.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap6KRSvSPeMZ"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMN2kizXPQKI"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "from sklearn import decomposition, ensemble\r\n",
        "\r\n",
        "import pandas, xgboost, numpy, textblob, string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRd-MKV7Oxne"
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(data['title'], data['domain'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqID2xRmPDSA"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\r\n",
        "tfidf_vect.fit(data['title'])\r\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\r\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR_e0MIfUCSx"
      },
      "source": [
        "from sklearn import *\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "\r\n",
        "mod = MultinomialNB()\r\n",
        "clf = mod.fit(xtrain_tfidf, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOAnp7o0T12w"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "predicted = mod.predict(xvalid_tfidf)\r\n",
        "print(\"Accuracy:\", accuracy_score(predicted, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArESKvzTglEi"
      },
      "source": [
        "import pickle\r\n",
        "# save the classifier\r\n",
        "with open('/content/drive/MyDrive/NB_classifier.pkl', 'wb') as fid:\r\n",
        "    pickle.dump(mod, fid)  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL7R7eD8fO6M"
      },
      "source": [
        "import pickle\r\n",
        "# load it again\r\n",
        "with open('/content/drive/MyDrive/NB_classifier.pkl', 'rb') as fid:\r\n",
        "    Nb_loaded = pickle.load(fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZAggaFFfa8q"
      },
      "source": [
        "Nb_loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwkhloKMUM3C"
      },
      "source": [
        "query = ['virat kohli']\r\n",
        "query_transform = tfidf_vect.transform(query)\r\n",
        "\r\n",
        "y_pred = Nb_loaded.predict(query_transform)\r\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jyu-hfwZGt_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}